确实是OpenAI一开始就是大佬云集+资金充裕想要干大事的

看了下GPT-1的几个作者，Alec Radford，他也是DCGAN的一作，15年发表，15k引用

Karthik Narasimhan在16年有一篇RL paper，1k多引用

Tim Salimans在2016年有一篇GAN文章9k引用

对比Transformer的几个作者，在Transformer之前都没有什么高引用文章，顶多两三百。

还有BERT一开始比GPT-1火，看有人说是因为BERT的完形填空确实比GPT-1的生成要简单（毕竟生成是extend，单边的），所以一开始BERT会有更好的效果
