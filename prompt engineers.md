# LARGE LANGUAGE MODELS ARE HUMAN-LEVEL PROMPT ENGINEERS

ABSTRACT 

By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the **quality of the prompt used to steer the model**, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose **Automatic Prompt Engineer1 (APE) for automatic instruction generation and selection**. In our method, we treat the instruction as the “program,” optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Extensive experiments show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 24/24 Instruction Induction tasks and 17/21 curated BIG-Bench tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts are able to improve few-shot learning performance (by simply prepending them to standard in-context learning prompts), find better zero-shot chain-ofthought prompts, as well as steer models toward truthfulness and/or informativeness.

通过根据自然语言指令的条件，大型语言模型（LLMs）已经展示出作为通用计算机的印象深刻的能力。然而，任务性能在很大程度上取决于用于**引导模型的提示的质量**，大多数有效的提示都是由人工制作的。受经典程序合成和人类提示工程方法的启发，我们提出了**自动提示工程师（APE）来实现自动指令生成和选择**。在我们的方法中，我们将指令视为“程序”，通过搜索由LLM提出的**一组指令候选项来优化，以最大化选择的得分函数**。为了评估所选指令的质量，我们评估了另一个LLM在遵循所选指令后的zero-shot性能。大量实验表明，我们自动生成的指令在24/24个指令归纳任务和17/21个精选的BIG-Bench任务上表现出比以前的LLM基线大得多的性能，并且在质量或性能上与由人工注释者生成的指令相比表现更好或相当。我们进行了大量的定性和定量分析，以探讨APE的性能。我们表明，APE工程师设计的提示能够提高少样本学习性能（仅需将其添加到标准上下文学习提示前），找到更好的zero-shot链式思维提示，并将模型引向真实性和/或信息性。

# 1 INTRODUCTION 

The combination of scale and attention-based architectures has resulted in language models possessing an unprecedented level of generality (Kaplan et al., 2020; Vaswani et al., 2017). These so-called “large language models” (LLMs) have shown remarkable, often superhuman, capabilities across a diverse range of tasks, including both zero-shot and few-shot setups (Brown et al., 2020; Srivastava et al., 2022). With generality, however, there comes a question of control: how can we make LLMs do what we want them to do?

规模和基于注意力的架构的结合导致了语言模型具有前所未有的通用性（Kaplan等人，2020；Vaswani等人，2017）。这些所谓的“大型语言模型”（LLMs）已经在各种任务中展现出卓越的能力，通常是超人的，包括zero-shot和few-shot设置（Brown等人，2020；Srivastava等人，2022）。然而，通用性也引出了一个问题：**我们如何使LLMs做我们想要的事情？**

To answer this question and steer LLMs toward desired behaviors, recent work has considered fine-tuning (Ouyang et al., 2022; Ziegler et al., 2019), in-context learning (Brown et al., 2020), and several forms of prompt generation (Gao, 2021), including both differentiable tuning of soft prompts (Qin & Eisner, 2021; Lester et al., 2021) and **natural language prompt engineering** (Reynolds & McDonell, 2021). The latter is of particular interest, as it provides a natural interface for humans to communicate with machines and may be of great relevance not only to LLMs but to other generalist models such as prompted image synthesizers (Rombach et al., 2022; Ramesh et al., 2022), for which public interest in prompt design and generation has also emerged (see Appendix A for examples).

为了回答这个问题并引导LLMs朝着所期望的行为方向发展，最近的研究考虑了微调（Ouyang等人，2022；Ziegler等人，2019）、上下文学习（Brown等人，2020）以及几种形式的提示生成（Gao，2021），包括可微调的软提示（Qin和Eisner，2021；Lester等人，2021）和**自然语言提示工程**（Reynolds和McDonell，2021）。后者尤其引人关注，因为它为人类提供了与机器沟通的自然界面，不仅对LLMs，还对其他通用模型，如提示图像生成器（Rombach等人，2022；Ramesh等人，2022）可能非常相关，对于这些模型，公众对提示设计和生成也产生了兴趣（请参阅附录A以查看示例）。

Behind this interest is the fact that plain language prompts do not always produce the desired results, even when those results are possible to produce with alternative instructions. Thus, human users must experiment with a wide range of prompts to elicit desired behaviors, as they have little knowledge of how compatible instructions are with a particular model. We can understand this by viewing LLMs as black-box computers that execute programs specified by natural language instructions: while they can execute a broad range of natural language programs, the way these programs are processed may not be intuitive for humans, and the quality of instruction can only be measured when executing these instructions on a downstream task (Sanh et al., 2022; Wei et al., 2021).

在这种兴趣背后的事实是，纯粹的语言提示并不总能产生期望的结果，即使这些结果可以通过替代指令来实现。因此，人类用户必须尝试各种各样的提示来引发所期望的行为，因为他们对特定模型与指令的兼容性了解甚少。我们可以将LLMs视为执行由自然语言指令指定的程序的**黑盒计算机**来理解这一点：尽管它们可以执行广泛的自然语言程序，但这些程序的**处理方式对人类来说可能并不直观**，指令的质量只能在执行这些指令时在下游任务上进行衡量（Sanh等人，2022；Wei等人，2021）。

To reduce the human effort involved in creating and validating effective instructions, we propose a novel algorithm using LLMs to generate and select instructions automatically. We call this problem **natural language program synthesis** and propose to address it as a black-box optimization problem using LLMs to generate and search over heuristically viable candidate solutions. In doing so, we leverage the generalist capabilities of LLMs in three ways. First, we use an LLM as an inference model (Ellis et al., 2021; Honovich et al., 2022) to generate instruction candidates based on a small set of demonstrations in the form of input-output pairs. Next, we guide the search process by computing a score for each instruction under the LLM we seek to control. Finally, we propose an iterative Monte Carlo search method where LLMs improve the best candidates by proposing semantically similar instruction variants. Intuitively, our algorithm asks LLMs to generate a set of instruction candidates based on demonstrations and then asks them to assess which instructions are more promising. We call our algorithm Automatic Prompt Engineer (APE). Our main contributions are:

为了减少创建和验证有效指令所需的人力工作，我们提出了一种**使用LLMs自动生成和选择指令的新算法**。我们将这个问题称为**自然语言程序合成**，并提议将其视为一个使用LLMs进行启发式可行候选解生成和搜索的黑盒优化问题。在这个过程中，我们利用LLMs的通用能力有三种方式。首先，我们将LLM用作推断模型（Ellis等人，2021；Honovich等人，2022），根据一小组以**输入-输出对形式呈现的演示生成指令候选项**。接下来，我们通过计算每个指令在我们试图控制的**LLM下的得分来引导搜索过程**。最后，我们提出了一个迭代的蒙特卡洛搜索方法，其中LLMs通过提出语义相似的指令变种来改进最佳候选项。从直观上讲，我们的算法要求LLMs根据演示生成一组指令候选项，然后询问它们哪些指令更有前途。我们将我们的算法称为**自动提示工程师（APE）**。我们的主要贡献包括：

• We frame instruction generation as natural language program synthesis, formulate it as a black-box optimization problem guided by LLMs, and propose both a naive and an iterative Monte Carlo search methods to approximate the solution. 

• Our proposed method, APE, achieves human-level performance on zero-shot learning with model-generated instructions on 24/24 Instruction Induction and 17/21 Big-Bench tasks. 

• We provide extensive qualitative and quantitative analyses exploring various facets of APE, and demonstrate applications of APE for improving few-shot learning, finding better zero-shot chain of thought prompts, and steering LLMs toward desired behaviors such as truthfulness and/or informativeness.

• 我们将指令生成框架视为自然语言程序合成，将其表述为由LLMs引导的黑盒优化问题，并提出了近似解的天真方法和迭代蒙特卡洛搜索方法。

• 我们提出的方法APE，在24/24个指令归纳任务和17/21个Big-Bench任务上，使用模型生成的指令实现了与人类水平相当的zero-shot学习性能。

• 我们提供了广泛的定性和定量分析，探讨了APE各个方面的应用，并展示了APE用于改进few-shot学习、找到更好的zero-shot思维链提示以及引导LLMs朝着真实性和/或信息性等期望行为的应用。

# 2 RELATED WORK 

Large Language Models Scaling up transformer-based language models in terms of model size, training data, and training compute has been shown to predictably improve performance on a wide range of downstream NLP tasks (Vaswani et al., 2017; Devlin et al., 2018; Brown et al., 2020). Many emergent abilities (Wei et al., 2022a) of LLMs have been discovered as a result of this scaling, including few-shot in-context learning, zero-shot problem solving, chain of thought reasoning, instruction following, and instruction induction (Cobbe et al., 2021; Wei et al., 2022b; Kojima et al.,2022; Sanh et al., 2022; Wei et al., 2021; Ouyang et al., 2022; Honovich et al., 2022). In this paper, we view LLMs as black-box computers that execute programs specified by natural language instructions and investigate how to control an LLM’s behavior using model-generated instructions. 

大型语言模型 事实证明，在模型大小、训练数据和训练计算方面扩展基于 Transformer 的语言模型可以显着提高各种下游 NLP 任务的性能（Vaswani 等人，2017 年；Devlin 等人， 2018；布朗等人，2020）。 由于这种扩展，LLM的许多新兴能力（Wei et al., 2022a）被发现，包括少样本上下文学习、零样本问题解决、思维链推理、指令跟踪和指令归纳。 Cobbe 等人，2021；Wei 等人，2022b；Kojima 等人，2022；Sanh 等人，2022；Wei 等人，2021；Ouyang 等人，2022；Honovich 等人，2022）。 在本文中，我们将 LLM 视为执行由自然语言指令指定的程序的黑盒计算机，并研究如何使用模型生成的指令来控制 LLM 的行为。

Prompting Engineering. Prompting offers a natural and intuitive interface for humans to interact with and use generalist models such as LLMs. Due to its flexibility, prompting has been widely used as a generic method for NLP tasks (Schick & Schütze, 2021; Brown et al., 2020; Sanh et al., 2022). However, LLMs require careful prompt engineering, either manually (Reynolds & McDonell, 2021) or automatically (Gao et al., 2021; Shin et al., 2020), as models do not seem to understand the prompts in the same way a human would (Webson & Pavlick, 2021; Lu et al., 2021). Though many successful prompt tuning methods perform optimization over a continuous space using gradient-based methods (Liu et al., 2021; Qin & Eisner, 2021; Lester et al., 2021), this becomes less practical with scale, as computing gradients becomes increasingly expensive and access to models shifts to APIs that may not provide gradient access. In our paper, we borrow components from discrete prompt search methods, such as prompt generation (Gao et al., 2021; Ben-David et al., 2021), prompt scoring (Davison et al., 2019) and prompt paraphrasing (Jiang et al., 2020; Yuan et al., 2021) to optimize instructions by searching directly in the natural language hypothesis space. As compared to this past work, which uses specialized models for each component and leans heavily on human templates, we show that the entire search can be conducted by a single LLM.

提示工程。提示提供了一个自然和直观的界面，供人类与通用模型如LLMs进行交互和使用。由于其灵活性，提示已被广泛用作自然语言处理任务的通用方法（Schick和Schütze，2021；Brown等人，2020；Sanh等人，2022）。然而，LLMs需要仔细的提示工程，可以是手动的（Reynolds和McDonell，2021）也可以是自动的（Gao等人，2021；Shin等人，2020），因为**模型似乎不以人类的方式理解提示**（Webson和Pavlick，2021；Lu等人，2021）。尽管许多成功的提示调整方法使用**基于梯度的方法**在连续空间上执行优化（Liu等人，2021；Qin和Eisner，2021；Lester等人，2021），但随着规模的扩大，这在实际中变得不太可行，因为**计算梯度的成本越来越昂贵**，对模型的访问转向了可能不提供梯度访问的API。在我们的论文中，我们借鉴了离散提示搜索方法的组件，如提示生成（Gao等人，2021；Ben-David等人，2021）、提示评分（Davison等人，2019）和提示改写（Jiang等人，2020；Yuan等人，2021），通过**直接在自然语言假设空间中搜索来优化指令**。与过去的工作相比，过去的工作使用每个组件的专门模型，并且在很大程度上依赖于人类模板，我们展示了整个搜索可以由单个LLM进行。

Program Synthesis。 Program synthesis involves the automatic search over a “program space” to find a program satisfying a particular specification (Gulwani et al., 2017). Modern program synthesis admits a wide variety of specifications, including input-output examples (Ellis et al., 2021; Wong et al., 2021) and natural language (Jain et al., 2022). The range of feasible program spaces to search over has also grown, from historically restrictive domain-specific languages to general-purpose programming languages (Austin et al., 2021). In contrast to prior approaches that require a suitable structured hypothesis space and library of components (Liang et al., 2010; Ellis et al., 2018), we leverage the structure provided by LLMs to search over the space of natural language programs. Using inference models is a standard practice to speed up the search by restricting the search space to a limited space of possible expressions (Menon et al., 2013; Lee et al., 2018; Devlin et al., 2017; Ellis et al., 2021). Inspired by this, we use LLMs as approximate inference models to generate program candidates based on a small set of demonstrations. Unlike classical program synthesis, our inference models do not require any training and generalize well to various tasks.

程序合成。程序合成涉及自动搜索“程序空间”，以找到满足特定规范的程序（Gulwani等人，2017）。现代程序合成允许各种各样的规范，包括输入-输出示例（Ellis等人，2021；Wong等人，2021）和自然语言（Jain等人，2022）。可搜索的程序空间范围也逐渐增大，从历史上的限制性领域特定语言扩展到通用编程语言（Austin等人，2021）。与以前需要适合的结构化假设空间和组件库的方法（Liang等人，2010；Ellis等人，2018）不同，我们利用**LLMs提供的结构来搜索自然语言程序的空间**。使用推理模型是加速搜索的标准做法，通过将搜索空间限制在可能表达的有限表达式空间内（Menon等人，2013；Lee等人，2018；Devlin等人，2017；Ellis等人，2021）。受此启发，我们将LLMs用作近似推理模型，根据一小组演示生成程序候选项。与传统的程序合成不同，我们的推理模型不需要任何训练，并且能够在各种任务中良好地泛化。

# 3 NATURAL LANGUAGE PROGRAM SYNTHESIS USING LLMS 

We consider a task specified by a dataset Dtrain = {(Q, A)} of input/output demonstrations sampled from population X , and a prompted model M. The goal of natural language program synthesis is to find a single instruction ρ such that, when M is prompted with the concatenation [ρ; Q] of instruction and a given input, M produces the corresponding output A. More formally, we frame this as an optimization problem, where we seek instruction ρ that maximizes the expectation of some per-sample score f(ρ, Q, A) over possible (Q, A):

我们考虑由从总体 X 采样的输入/输出演示的数据集 Dtrain = {(Q, A)} 和提示模型 M 指定的任务。自然语言程序综合的目标是找到单个指令 ρ ，使得 当 M 被提示连接 [ρ; 指令 Q] 和给定输入，M 产生相应的输出 A。更正式地说，我们将其视为一个优化问题，其中我们寻求指令 ρ 最大化某些每个样本分数 f(ρ, Q, A) 的期望 超过可能（Q，A）：

![image-20231009034145324](C:\Users\think\AppData\Roaming\Typora\typora-user-images\image-20231009034145324.png)

Note that in general, Q may be the empty string, such that we are optimizing ρ as a prompt that directly produces outputs {A}. While this task has been widely attempted by humans, we have little knowledge of how compatible any particular instruction is with model M. Thus, we propose to treat this human-intractable question as a black-box optimization process guided by LLMs. Our algorithm, APE, uses LLMs in each of two key components, proposal and scoring. As shown in Figure 1 and summarized in Algorithm 1, APE first proposes a few candidate prompts, and then filters/refines the candidate set according to a chosen score function, ultimately choosing the instruction with the highest score. We discuss options for proposal and scoring next. 

请注意，一般来说，Q 可能是空字符串，因此我们将 ρ 优化为直接产生输出 {A} 的提示。 虽然人类已经广泛尝试了这项任务，但我们对任何特定指令与模型 M 的兼容性知之甚少。因此，我们建议将这个人类难以解决的问题视为由LLM指导的黑盒优化过程。 我们的算法 APE 在两个关键组成部分（提案和评分）中均使用了LLM。 如图 1 所示，并在算法 1 中进行了总结，APE 首先提出一些候选提示，然后根据选定的评分函数过滤/细化候选集，最终选择得分最高的指令。 接下来我们讨论提案和评分的选项。

## 3.1 INITIAL PROPOSAL DISTRIBUTIONS 

Due to the infinitely large search space, finding the right instruction can be extremely difficult, which has rendered natural language program synthesis historically intractable. Recent progress in NLP has shown language models are very good at generating diverse natural language text. Therefore, we consider leveraging a pretrained LLM to propose a good set U of candidate solutions that will guide our search procedure. While random samples from LLMs are unlikely to produce the desired (Q, A) pairs, we can instead ask the LLM to approximately infer the most likely instructions with a high score, given the input/output demonstrations; i.e., to approximately sample from P(ρ | Dtrain, f(ρ) is high).

由于搜索空间无限大，找到正确的指令可能非常困难，这使得自然语言程序合成在历史上难以处理。最近在自然语言处理领域的进展表明，语言模型非常擅长生成多样的自然语言文本。因此，我们考虑利用**预训练的LLM来提出一个良好的候选解集U**，以指导我们的搜索过程。虽然从LLMs中随机采样不太可能产生所需的（Q，A）对，但我们可以**要求LLMs大致推断出在给定输入/输出演示时分数较高的最可能的指令**，即从P(ρ | Dtrain, f(ρ)高)中大致采样。

Forward Mode Generation. We consider two approaches to generate high-quality candidates from P(ρ | Dtrain, f(ρ) is high). First, we adopt an approach based on “forward” mode generation by translating this distribution P(ρ | Dtrain, f(ρ) is high) into words. For example, in our instruction induction experiments (Subsection 4.1), we follow Honovich et al. (2022) and prompt the LLM using Figure 2 (Top).

正向模式生成 我们考虑两种从P(ρ | Dtrain, f(ρ)高)中生成高质量候选项的方法。首先，我们采用基于“正向”模式生成的方法，将分布P(ρ | Dtrain, f(ρ)高)转化为单词。例如，在我们的指令归纳实验中（第4.1小节），我们遵循Honovich等人（2022）的方法，并使用图2（顶部）中的提示来引导LLM。

Reverse Mode Generation. Although the “forward” model works out of the box for most of the pretrained LLMs, translating P(ρ | Dtrain, f(ρ) is high) into words requires custom engineering across different tasks. This is because while instructions are typically found in the beginning of passages, the “forward” model only generates text from left to right, which requires the instruction to be predicted at the end of the prompt. Therefore, we desire a more flexible approach such that the instruction can be anywhere in the text. To address this, we consider “reverse” mode generation, which uses an LLM with infilling capabilities—e.g., T5 (Raffel et al., 2020), GLM (Du et al., 2022), and InsertGPT (Bavarian et al., 2022)—to infer the missing instructions. Our “reverse” model directly samples from P(ρ | Dtrain, f(ρ) is high) by filling in the blank. We show an example of the such template in Figure 2 (Middle).

反向模式生成 尽管“正向”模型对于大多数预训练LLMs来说可以立即使用，但将P(ρ | Dtrain, f(ρ)高)转化为单词在不同任务之间需要定制工程。这是因为尽管指令通常位于段落的开头，但“正向”模型只会从左到右生成文本，这要求指令在提示的末尾被预测。因此，我们需要一种**更灵活的方法，使指令可以出现在文本的任何位置**。为了解决这个问题，我们考虑“反向”模式生成，它使用具有填充功能的LLM，例如T5（Raffel等人，2020），GLM（Du等人，2022）和InsertGPT（Bavarian等人，2022），以推断缺失的指令。我们的“反向”模型通过填充空白直接从P(ρ | Dtrain, f(ρ)高)中采样。我们在图2（中部）显示了一个此类模板的示例。

Customized Prompts. Note that depending on the score function being used, there may exist more appropriate prompts than the samples above. For example, in our TruthfulQA experiments, we start with the human-designed instructions from the original dataset (Lin et al., 2022) and ask the the “reverse” model to propose initial instruction samples that fit the missing context (Figure 2 (Bottom)).

自定义提示。请注意，根据所使用的分数函数，可能存在比上述示例更合适的提示。例如，在我们的TruthfulQA实验中，我们首先使用原始数据集（Lin等人，2022）中的人工设计指令，并要求“反向”模型提出适合缺失上下文的初始指令样本（图2（底部））。

## 3.2 SCORE FUNCTIONS

To cast our problem as black-box optimization, we choose a score function that accurately measures the alignment between the dataset and the data the model generates. In our instruction induction experiments, we consider two potential score functions, described below. In the TruthfulQA experiments, we focused primarily on automated metrics proposed in Lin et al. (2022), similar to the execution accuracy. In each case, we evaluate the quality of a generated instruction using Equation (1), and take the expectation over a held-out test dataset Dtest.

为了将我们的问题形式化为黑盒优化问题，我们选择一个可以准确衡量数据集与模型生成数据之间对齐度的分数函数。在我们的指令归纳实验中，我们考虑了两种潜在的分数函数，如下所述。在TruthfulQA实验中，我们主要关注了Lin等人（2022）提出的自动评估指标，类似于执行准确性。在每种情况下，我们使用方程（1）评估生成指令的质量，并对一个留出的测试数据集Dtest进行期望。

Execution accuracy First, we consider evaluating the quality of an instruction ρ using the execution accuracy metric proposed by Honovich et al. (2022), which we denote as fexec. In most cases,execution accuracy is simply defined as the 0-1 loss, f(ρ, Q, A) = 1 [M([ρ; Q]) = A]. On some tasks, execution accuracy takes into account invariants; e.g., it may be an order invariant set matching loss, as described in Appendix A of Honovich et al. (2022).

执行准确性
首先，我们考虑使用由Honovich等人（2022）提出的执行准确性度量来评估指令ρ的质量，我们将其表示为fexec。在大多数情况下，执行准确性被简单地定义为0-1损失，即f(ρ, Q, A) = 1 [M([ρ; Q]) = A]。在某些任务中，执行准确性考虑了不变性；例如，它可以是一种考虑了顺序不变性的集合匹配损失，如Honovich等人（2022）的附录A中所述。

Log probability We further consider a softer probabilistic score function, which we hypothesize might improve optimization by providing a more fine-grained signal when searching over low-quality instruction candidates. In particular, we consider the log probability of the desired answer given the instruction and question under the target model M, which on a per sample basis, is log P(A | [ρ; Q]).

对数概率 我们进一步考虑了一个更加柔和的概率分数函数，我们假设这可能会在搜索低质量指令候选项时提供更细粒度的信号，从而改善优化。具体来说，我们考虑了在目标模型M下，对于每个样本，给定指令和问题的情况下所需答案的对数概率，即log P(A | [ρ; Q])。

Efficient score estimation Estimating the score by computing the score over the entire training dataset for all instruction candidates can be expensive. To reduce the computation cost, we adopt a filtering scheme where a promising candidate receives more computation resources while a lowquality candidate receives less computation. It can be achieved by using a multi-stage computation strategy on lines 2-9 Algorithm 1. We first evaluate all candidates with a small subset of the training dataset. For the candidates with a score greater than a certain threshold, we sample and evaluate a new non-overlapping subset from the training dataset to update the moving average of the score. Then, we repeat this process until a small set of candidates is left, which are evaluated on the entire training dataset. This adaptive filtering scheme significantly improves the computation efficiency by keeping the exact computation costs for the high-quality samples and drastically reducing the computation costs for low-quality candidates. We note that a similar score estimation scheme has been used in previous works (Li et al., 2022; Maclaurin & Adams, 2015).

高效的分数估计 通过计算所有指令候选项在整个训练数据集上的分数来估计分数可能会很昂贵。为了降低计算成本，我们采用了一个过滤方案，其中有前景的候选项获得更多的计算资源，而低质量的候选项获得更少的计算资源。这可以通过Algorithm 1的2-9行上的多阶段计算策略来实现。我们首先使用训练数据集的一个小子集评估所有候选项。对于得分高于一定阈值的候选项，我们从训练数据集中采样并评估一个新的非重叠子集，以更新得分的移动平均值。然后，我们重复这个过程，直到只剩下一小组候选项，这些候选项在整个训练数据集上进行评估。这种自适应过滤方案通过保持高质量样本的精确计算成本并大幅降低低质量候选项的计算成本，显著提高了计算效率。我们注意到，类似的分数估计方案在先前的工作中也已经使用过（Li等人，2022；Maclaurin和Adams，2015）。

## 3.3 ITERATIVE PROPOSAL DISTRIBUTIONS

Despite our attempt to directly sample high-quality initial instruction candidates, it could be the case that the method described in Subsection 3.1 fails to produce a good proposal set U, either because it lacks of diversity or does not contain any candidates with a suitably high score. In case of such challenges, we explore an iterative process for resampling U.

尽管我们尝试直接采样高质量的初始指令候选项，但在某些情况下，第3.1小节描述的方法可能**无法生成良好的提案集U，可能是因为缺乏多样性，或者没有包含得分适当高的候选项**。在面临这些挑战的情况下，我们探索了一种用于重新采样U的迭代过程。

Iterative Monte Carlo Search Instead of only sampling from the initial proposal, we consider exploring the search space locally around the current best candidates. This allows us to generate new instructions that are more likely to be successful. We call this variant iterative APE. At each stage, we evaluate a set of instructions and filter out candidates with low scores. Then, an LLM is asked to generate new instructions similar to those with high scores. We provide the prompt used for resampling in Figure 3. Figure 6 (Right) shows that although this approach improves the overall quality of the proposal set U, the highest scoring instruction tends to remain the same with more stages. We conclude iterative generation provides marginal improvement over the relative simplicity and effectiveness of the generative process described in Subsection 3.1. Therefore, we use APE without iterative search as default unless otherwise stated.

迭代蒙特卡洛搜索 我们**不仅仅从初始提案中进行采样**，还考虑在**当前最佳候选项周围局部探索搜索空间**。这允许我们生成更有可能成功的新指令。我们将这个变体称为迭代APE。在每个阶段，我们评估一组指令并过滤得分低的候选项。然后，我们要求一个LLM生成与高分指令类似的新指令。我们在图3中提供了用于重新采样的提示。图6（右侧）显示，尽管这种方法改善了提案集U的整体质量，但得分最高的指令在更多阶段中往往保持不变。我们得出结论，迭代生成相对于第3.1小节中描述的生成过程的相对简单性和有效性提供了**较小的改进**。因此，除非另有说明，否则我们使用没有迭代搜索的APE作为默认方法。

<img src="D:\coursefile\LLM\typorapic\image-20231009045124696.png" alt="image-20231009045124696" style="zoom:67%;" />

# 4 LARGE LANGUAGE MODELS ARE HUMAN-LEVEL PROMPT ENGINEERS

This section examines how APE can guide LLMs to desired behaviors. We investigate from four perspectives: zero-shot performance, few-shot in-context learning performance, zero-shot chain-ofthought reasoning, and truthfulness. Our experiments show that APE can find prompts that improve task performance, performing equal to or even better than those authored by humans. APE also often produces insightful tricks for how to best prompt language models that can be successfully transferred to new tasks (see Section 4.3).

本节将考察APE如何引导LLMs朝着期望的行为方向发展。我们从四个角度进行调查：zero-shot性能、few-shot的上下文学习性能、zero-shot的思维链推理和真实性。我们的实验证明，**APE可以找到可以提高任务性能的提示，表现与甚至优于人类作者编写的提示**。APE经常产生有洞察力的技巧，用于如何最好地引导语言模型，这些技巧可以成功地转移到新任务上（见第4.3节）。

## 4.1 INSTRUCTION INDUCTION

We assess the effectiveness of zero-shot and few-shot in-context learning on 24 instruction induction tasks proposed in Honovich et al. (2022). The tasks span many facets of language understanding, from simple phrase structure to similarity and causality identification. We provide a detailed descriptions of each task in Appendix B. For each task, we sample five input-output pairs from the training data and select the best instruction using algorithm 1. Then, we evaluate the quality of the instruction by executing the instruction on InstructGPT 3 . We repeat our experiments five times with different random seeds to report the mean and standard deviation. The exact templates for our experiments can be found in Appendix (Table 5).

我们在Honovich等人（2022）提出的24个指令归纳任务上评估了zero-shot和few-shot的上下文学习的有效性。这些任务涵盖了语言理解的许多方面，从简单的短语结构到相似性和因果关系识别。我们在附录B中提供了每个任务的详细描述。对于每个任务，我们从训练数据中随机选择五个输入-输出对，并使用算法1选择最佳指令。然后，我们**通过在InstructGPT 3上执行指令来评估指令的质量**。我们使用不同的随机种子重复了五次实验，以报告均值和标准差。我们实验的确切模板可以在附录（表5）中找到。

Zero-shot Learning We compare our method against two baselines: human prompt engineers (Human)4 and the model-generated instruction algorithm proposed by Honovich et al. (2022). This algorithm can be thought of as a greedy version of APE, without a search and selection process; thus, we refer to it as “Greedy”. Figure 4 shows the zero-shot performance of InstructGPT using human instructions and model generated instructions. Our algorithm outperforms “Greedy” on every task and achieves equal or better than human performance on 24 of 24 tasks. Moreover, the Interquartile Mean (IQM) (Agarwal et al., 2021) across all 24 tasks in Figure 1 suggests that APE with InstructGPT outperforms human-engineered prompts, obtaining an IQM of 0.810 vs humans’ 0.749. We summarize the instruction selected by APE for each task in Appendix (Table 12).

zero-shot学习 我们将我们的方法与两个基线进行比较：人工提示工程师（Human）和Honovich等人（2022）提出的**模型生成指令算法。这个算法可以被看作是APE的贪婪版本，没有搜索和选择过程**；因此，我们将其称为“Greedy”。图4显示了使用人工指令和模型生成指令的InstructGPT的zero-shot性能。我们的算法在每个任务上都优于“Greedy”，并在24个任务中的24个任务中达到了与人类性能相等或更好的水平。此外，图1中所有24个任务的四分位平均（IQM）（Agarwal等人，2021）表明，在InstructGPT的情况下，APE优于人工设计的提示，获得了IQM值为0.810，而人类的IQM值为0.749。我们在附录（表12）中总结了APE为每个任务选择的指令。

Few-shot In-context Learning We evaluated APE-generated instructions in few-shot in-context learning, where we insert the instruction before the in-context demonstrations. Those instructions are selected based on zero-shot execution accuracy, and we denote this setting as “Instruction + In-context” in Figure 8. As shown in Figure 8, adding an instruction achieves a comparable or better test performance than the standard in-context learning performance on 21 of 24 tasks. Counterintuitively, adding in-context examples for Rhymes, Large Animal, and Second Letters hurts model performance. We conjecture that it may be because the selected instructions overfit the zero-shot learning scenario and thus do not perform well on the few-shot case. Therefore, we experiment using few-shot execution accuracy as the selection metric. Figure 14 shows that the few-shot metric achieves comparable or slightly better than the zero-shot metric except for Rhymes. To have an intuitive understanding of what is happening, we provide a qualitative analysis in Appendix C.1.

few-shot上下文学习 我们在few-shot上下文学习中评估了APE生成的指令，其中我们将指令插入到上下文演示之前。这些指令是基于zero-shot执行准确性选择的，我们在图8中将此设置称为“Instruction + In-context”。如图8所示，添加指令在24个任务中的21个任务上实现了与标准上下文学习性能相媲美或更好的测试性能。出乎意料的是，在Rhymes、Large Animal和Second Letters添加上下文示例会损害模型性能。我们推测这可能是因为所选指令过度拟合zero-shot学习情境，因此在few-shot情况下表现不佳。因此，我们尝试使用few-shot执行准确性作为选择度量标准。图14显示，few-shot指标在除Rhymes外的情况下表现相当或稍好于zero-shot指标。为了更直观地了解发生了什么，我们在附录C.1中提供了定性分析。

## 4.2 BIGBENCH

To see whether APE can be applied to more challenging tasks, we propose and curate BIG-Bench Instruction Induction (BBII), a clean and tractable subset of 21 tasks that have a clear, human-written instruction that can be applied to all examples in the dataset. The selected tasks cover many facets of language understanding and includes all nine such problems from the BigBench-Hard Subset (Suzgun et al., 2022). In particular, it includes emotional understanding, context-free question answering, reading comprehension, summarization, algorithms, and various reasoning tasks (e.g., arithmetic, commonsense, symbolic, and other logical reasoning tasks). We provide a detailed description of the task and our selection criteria in Appendix B.

为了看看APE是否可以应用于更具挑战性的任务，我们提出并精选了**BIG-Bench指令归纳**（BBII），这是一个干净且可管理的包含21个任务的子集，这些任务具有明确的人工编写指令，可应用于数据集中的所有示例。所选任务涵盖了语言理解的许多方面，包括BigBench-Hard子集（Suzgun等人，2022）中的所有九个问题。特别是，它包括情感理解、无上下文问答、阅读理解、摘要、算法以及各种推理任务（例如算术、常识、符号和其他逻辑推理任务）。我们在附录B中提供了任务的详细描述和我们的选择标准。

对于每个任务，我们使用InstructGPT的反向模式生成来生成一组指令候选项，并根据它们的执行准确性对指令进行排名。然后，我们在InstructGPT上执行所选的指令，计算测试集上的zero-shot性能，并将其与默认人工提示进行比较。如附录表6所示，APE在21个任务中的17个任务上实现了与默认人工提示相媲美或更好的性能。

For each task, we used the reverse mode generation of InstructGPT to generate a set of instruction candidates and ranked the instructions based on their execution accuracy. Then, we executed the selected instruction on InstructGPT to compute the zero-shot performance on the test set and compared it with the default human prompt. As shown in Appendix Table 6, APE achieves comparable or better performance than the default human prompt on 17 out of 21 tasks.

## 4.3 ZERO-SHOT CHAIN OF THOUGHT

Chain-of-thought reasoning has been shown to dramatically improve the ability of LLMs to complete complex reasoning tasks, such as solving math problems that require multiple steps. Early works (Nye et al., 2021; Betz et al., 2021; Wei et al., 2022b) on chain-of-thought used fine-tuning or in-context learning to get LLMs to show their work for such problems. One of the most influential recent works of prompt engineering was the discovery (Kojima et al., 2022) that LLMs could be made to give chain-of-thoughts simply by prepending “Let’s think step by step.” to the beginning of the LLM’s response. Known as Zero-Shot-CoT, this prompting strategy improves the zero-shot performance of InstructGPT on MultiArith (Roy & Roth, 2016) from 17.7 to 78.7 and improves performance on GSM8K(Cobbe et al., 2021) from 10.4 to 40.7. As shown in Table 7, Kojima et al. (2022) found their prompt was the best performing out of at least nine human-designed prompts.

**链式思维推理**已经被证明可以显著提高LLMs完成复杂推理任务的能力，例如解决需要多个步骤的数学问题。早期的链式思维研究（Nye等人，2021；Betz等人，2021；Wei等人，2022b）使用了微调或上下文学习来让LLMs为这类问题展示他们的工作过程。最近一项最具影响力的提示工程工作是发现（Kojima等人，2022），只需在LLMs的响应开头添加“让我们一步一步地思考。”，就可以让LLMs给出链式思维。这被称为zero-shot-CoT，这种提示策略将InstructGPT在MultiArith（Roy＆Roth，2016）上的zero-shot性能从17.7提高到78.7，并将GSM8K（Cobbe等人，2021）上的性能从10.4提高到40.7。如表7所示，Kojima等人（2022）发现他们的提示是至少九个人工设计提示中表现最好的。

我们使用APE自动搜索了Kojima等人（2022）中使用的一组任务中最佳答案前缀。我们优化这个提示的方法受到了Zelikman等人（2022）的启发。首先，我们使用“让我们一步一步地思考。”来生成问题和思考步骤的数据集。然后，我们删除了任何答案不正确的数据点。最后，我们使用APE来找到以“让我们”开头的提示，最大化这些正确思考步骤的可能性。有关用于提示生成和评估的模板，请参见表5。APE生成的提示是“让我们一步一步地思考，以确保我们有正确的答案。”这个生成的提示进一步将MultiArith上的性能从78.7提高到82.0，并将GSM8K上的性能从40.7提高到43.0。我们认为这个通用的工作流程代表了APE的一个常见用例，其中提示工程师使用APE来优化其现有模板的部分以提高性能。有关该提示在其他推理任务上的性能详细信息，请参见图10。

We used APE to automatically search for the best answer-prefix across the suite of tasks used in Kojima et al. (2022). Our approach to optimizing this prompt was inspired by Zelikman et al. (2022). First, we generate a dataset of questions and reasoning steps generated using InstructGPT with “Let’s think step by step.” Then, we remove any data points that had incorrect answers. Finally, we use APE to find a prompt starting with “Let’s” that maximizes the likelihood of these correct reasoning steps. See Table 5 for the template used for prompt generation and evaluation. APE produces the prompt “Let’s work this out in a step by step way to be sure we have the right answer.” This generated prompt further improves performance from 78.7 to 82.0 on MultiArith and from 40.7 to 43.0 on GSM8K. We believe this general workflow represents a common use-case for APE where prompt engineers use APE to optimize parts of their exiting templates to improve performance. See Figure 10 for details on the performance of this prompt on other reasoning tasks.

## 4.4 TRUTHFULQA

We apply our method on TruthfulQA (Lin et al., 2022) to see how APE-generated instructions can steer an LLM to generate answers with different styles, and study the trade-off between truthfulness and informativeness. Borrowing the metrics from the original paper, we use APE to the learn instructions that maximize three metrics: truthfulness (% True), informativeness (% Info), and a combination of both (%True + %Info). Lin et al. (2022) used human evaluation to assess the model performance, but they found their automated metrics align with human prediction over 90% of the time. In our experiments, we rely on their fine-tuned GPT-judge and GPT-info to evaluate the scores.

我们在TruthfulQA（Lin等人，2022）上应用我们的方法，以查看APE生成的指令如何引导LLM生成不同风格的答案，并研究真实性和信息性之间的权衡。借鉴原论文的度量标准，我们使用APE来学习最大化三个度量标准的指令：真实性（% True），信息性（% Info）以及真实性和信息性的组合（% True + % Info）。Lin等人（2022）使用人工评估来评估模型性能，但他们发现他们的自动化度量标准在超过90%的情况下与人类预测一致。在我们的实验中，我们依赖于他们精细调整的GPT-judge和GPT-info来评估得分。

Prompt Engineering in TruthfulQA. We want to stress that the TruthfulQA dataset is intended to test pretrained models in zero-shot settings. Our results are not in any way compatible with the original benchmarks. Because we have optimized the instructions using a small portion of the question and answer pairs as training demonstrations, our results are not “true few-shot learning” (Perez et al., 2021). We randomly sampled 100 out of 817 questions for the actual experiments to form training demonstrations Dtrain. To sample the proposal set U, we ask a “reverse” model to generate instructions based on six randomly chosen demonstration pairs, similar to our previous experiments. Unlike in Instruction Induction, in TruthfulQA, we aim to find a single best instruction prompt that works well across all 38 categories of questions spanning health, law, politics, and fiction. It is worth noting all our generated instructions are very generic, e.g., “You will be asked a series of questions. For each question, you must either answer the question or decline to answer, in which case you must state that you have no comment”, and do not contain any examples from the dataset.

在TruthfulQA中的提示工程。我们要强调，TruthfulQA数据集旨在测试zero-shot设置中的预训练模型。我们的结果与原始基准测试结果完全不兼容。因为我们使用了一小部分问题和答案对作为训练演示来优化指令，所以我们的结果不是“真正的few-shot学习”（Perez等人，2021）。我们随机抽取了817个问题中的100个问题，以形成训练演示Dtrain。为了抽样提案集U，我们要求一个“反向”模型基于随机选择的六对演示生成指令，与我们之前的实验类似。与指令归纳不同，在TruthfulQA中，我们的目标是找到一个适用于跨足健康、法律、政治和小说等38个问题类别的单个最佳指令提示。值得注意的是，我们生成的所有指令都非常通用，例如：“您将被问一系列问题。对于每个问题，您必须回答问题或拒绝回答，如果您拒绝回答，您必须声明您无评论”，不包含来自数据集的任何示例。

Truthfulness vs Informativeness Trade-off. We found that APE outperforms the humanengineered prompt with only 200 candidates proposed by InstructGPT (175B), as seen in Figure 5. We compared our generated prompt with the “help” prompt from Lin et al. (2022). The training and test performance are shown in Figure 5(a)-(b). We found that choosing the top 10 of 200 candidates on the training set generalizes well to the test set. We report the average performance across the top 10 instructions for the three metrics. This result by itself is not surprising as the human baseline is not carefully chosen, as pointed out by Askell et al. (2021). However, we found that the instructions discovered by APE can achieve very high truthfulness with answers such as “No comment,” but these answers provide little information. We used our top candidates to further investigate the trade-off between truthfulness and informativeness. We visualize the top 10 proposed samples across the three metrics on the truthfulness-informative plots shown in Figure 5(c) and Figure 5(d). While APE achieves over 40% accuracy in providing both true and informative answers (v.s. 30% by the “help” prompt from humans), the instructions discovered tend to target the two ends of this %true-%info Pareto frontier.

真实性与信息性的权衡。我们发现APE在只使用InstructGPT（175B）提出的200个候选项的情况下胜过了人工设计的提示，如图5所示。我们将我们生成的提示与Lin等人（2022）的“帮助”提示进行了比较。训练和测试性能如图5(a)-(b)所示。我们发现，在训练集上选择前10个200个候选项在测试集上具有很好的泛化能力。我们报告了三个度量标准的前10个指令的平均性能。这个结果本身并不令人惊讶，因为人工基线并没有经过精心选择，正如Askell等人（2021）所指出的那样。然而，我们发现APE发现的指令可以实现非常高的真实性，比如“无评论”，但这些答案提供的信息很少。我们使用我们的前几个候选项进一步研究真实性和信息性之间的权衡。我们在图5(c)和图5(d)中显示了真实性-信息性图上三个度量标准的前10个建议样本。尽管APE在提供真实和信息丰富的答案方面取得了超过40%的准确率（而人类“帮助”提示只有30%），但发现的指令倾向于瞄准%真实-%信息帕累托前沿的两端。

# 5 QUANTITATIVE ANALYSIS

In this section, we conduct quantitative analyses to better understand the three main components of our method: proposal distribution, score functions, and iterative search. Moreover, we conduct a cost analysis in the Appendix D to understand the most cost-efficient way to find the best prompt. We observe the larger and more powerful language models are more cost-effective for generating the best prompt despite a higher per-token cost.

在本节中，我们进行**定量分析**，以更好地理解我们方法的三个主要组成部分：**提案分布、评分函数和迭代搜索**。 此外，我们在附录 D 中进行了成本分析，以了解找到最佳提示的最具成本效益的方法。 我们观察到，尽管每个标记的成本更高，但更大、更强大的语言模型在生成最佳提示方面更具成本效益。

## 5.1 LLMS FOR PROPOSAL DISTRIBUTION

How does the proposal quality change as we increase the model size? To understand how the model size affects the quality of the initial proposal distribution, we examine eight different models5 available via the OpenAI API. To assess the quality of the proposal distribution, we generate 250 instructions per model and compute the execution accuracy on 50 test data points. We visualize the survival function (percentage of instructions with test accuracy greater than a certain threshold) and the histogram of test accuracy for a simple task (i.e., Pluralization) in Figure 6 (a) and include a similar plot for a more challenging task (Start With) in the Appendix (Figure 28). As shown in both figures (and unsurprisingly), larger models tend to produce better proposal distributions than smaller ones, as do the models that were fine-tuned to follow human instructions. On the simple task, all instructions generated by the best model, InstructGPT (175B), have reasonable test accuracy. In contrast, half of the instructions are off-topic and perform poorly on the more challenging task.

随着模型大小的增加，提案质量会如何改变？为了了解模型大小如何影响初始提案分布的质量，我们研究了OpenAI API提供的八个不同模型。为了评估提案分布的质量，我们对每个模型生成250个指令，并在50个测试数据点上计算执行准确度。我们在图6(a)中可视化了生存函数（具有大于某个阈值的测试准确度的指令百分比）和一个简单任务（即复数化）的测试准确度直方图，并在附录（图28）中包括了一个更具挑战性任务（以“开始”开头）的类似情节。如图所示（并不奇怪），较大的模型通常比较小的模型产生更好的提案分布，而那些被微调以遵循人类指令的模型也是如此。在简单任务中，由最佳模型InstructGPT（175B）生成的所有指令都具有合理的测试准确度。相比之下，在更具挑战性的任务中，有一半的指令与主题无关，并在性能上表现不佳。

![image-20231009045743359](D:\coursefile\LLM\typorapic\image-20231009045743359.png)

## 5.2 LLMS FOR SELECTION

Does proposal quality matter under selection? If we sample more instructions from the LLMs, then it becomes more likely for us to find better instructions. To verify this hypothesis, we increase the sample size from 4 to 128 and evaluate the test accuracy change. Figure 7 (Left) shows a monotonically increasing trend with a diminishing return, as human-level performance is achieved with 64 instruction samples. Thus, we choose 50 as our default sample size. Under this configuration, we investigate how the proposal distribution affects the test accuracy of the best instruction selected by our algorithm. Figure 1(b) shows that though the small models may be less likely to generate good instructions, they nonetheless generate some good ones if we sample enough candidates. Therefore,we still find promising instructions with a small model by running our selection algorithm, explaining why our method outperforms the greedy approach Honovich et al. (2022) across all eight models.

Which scoring function is better? We compute the correlation between the test accuracy and two metrics on 24 instruction induction tasks to study how good our proposed metrics are. We generate 250 instructions per task using InstructGPT (175B) in “forward” mode and compute the metric score and test accuracy on 10 test data points. We visualize the Spearman correlation between the test accuracy and two metrics. Figure 7 (Middle) shows that the execution accuracy aligns better with the test performance across the tasks. Thus, we choose it as our default metric unless otherwise stated.

提案质量是否在选择过程中重要？如果我们从LLMs中抽取更多的指令，那么我们更有可能找到更好的指令。为了验证这个假设，我们将**样本大小从4增加到128**，并评估测试准确度的变化。如图7（左）所示，随着样本量的增加，趋势呈单调增加的趋势，但收益递减，因为在64个指令样本时达到了人类水平的性能。因此，我们选择50作为默认的样本大小。在这种配置下，我们研究提案分布如何影响由我们的算法选择的最佳指令的测试准确度。图1(b)显示，尽管小型模型可能不太可能生成好的指令，但如果我们抽样足够的候选项，它们仍然会生成一些好的指令。因此，通过运行我们的选择算法，我们仍然可以找到有前途的指令，解释了为什么我们的方法在所有八个模型上都优于贪婪方法Honovich等人（2022）。

哪种评分函数更好？我们计算了在24个指令归纳任务上测试准确度和两个度量标准之间的相关性，以研究我们提出的度量标准有多好。我们使用InstructGPT（175B）在“正向”模式下为每个任务生成250个指令，并在10个测试数据点上计算度量分数和测试准确度。我们可视化了测试准确度和两个度量标准之间的Spearman相关性。图7（中）显示，在各项任务中，执行准确度更好地与测试性能相一致。因此，我们选择它作为默认的度量标准，除非另有说明。

## 5.3 ITERATIVE MONTE CARLO SEARCH

Does Iterative Search improve the instruction quality? We visualize the survival function and histogram of test accuracy on the “Passivization” task in Figure 6 (Right) and include five more tasks in the Appendix. The survival plot shows that the curves increase as the round goes up, which suggests that iterative search does result in a higher-quality proposal set. However, we observe diminishing returns to further selection rounds as the quality seems to stabilize after three rounds.

Do we need Iterative Search? We compare APE and iterative APE on six tasks6 . As shown in Figure 7, the iterative search marginally improves performance on tasks where APE underperforms humans but achieves similar performance on the other tasks. This is consistent with our hypothesis that iterative search would be most useful on tasks where generating a good initial U is challenging.

迭代搜索是否会提高指令质量？我们在图6（右）中可视化了“被动化”任务的生存函数和测试准确度的直方图，并在附录中包含了五个更多的任务。生存曲线显示，随着迭代轮数的增加，曲线逐渐上升，这表明迭代搜索确实会产生更高质量的提案集。然而，我们观察到在三轮后质量似乎趋于稳定，进一步选择轮次带来的效益递减。

我们是否需要迭代搜索？我们在六个任务上比较了APE和迭代APE。如图7所示，迭代搜索在APE表现不如人类的任务上略有改善，但在其他任务上实现了类似的性能。这与我们的假设一致，即迭代搜索在生成良好的初始U较为困难的任务上最有用。

# 6 CONCLUSION

Large language models can be seen as general-purpose computers that execute programs specified by natural language prompts. We automate the prompt engineering process by formulating it as a black-box optimization problem, which we propose to solve using efficient search algorithms guided by LLMs. Our method achieves human-level performance on various tasks with minimum human inputs. As recent LLMs demonstrate an impressive ability to follow human instruction, we expect many future models, including those for formal program synthesis, to have a natural language interface. This work builds the foundation to control and steer generative artificial intelligence.

大型语言模型可以看作是**执行由自然语言提示指定的程序的通用计算机**。我们将**提示工程过程自动化，将其构建为一个黑盒优化问题**，并建议使用由LLMs指导的高效搜索算法来解决这个问题。我们的方法在各种任务上实现了接近人类水平的性能，几乎没有人工干预。由于最近的LLMs展示出了遵循人类指示的惊人能力，我们预计未来的许多模型，包括用于形式程序合成的模型，都将具有自然语言界面。这项工作为控制和引导生成型人工智能奠定了基础。